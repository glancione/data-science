{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41c73ca",
   "metadata": {},
   "source": [
    "# Exercises - Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ecce3",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2b8e0-0ba0-4223-9405-99809ab94537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import interp\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c661389",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cbce5-ef75-4ef6-b4a9-ec71a798d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('./exercises-churn-prediction-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d915a",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = churn_df.columns.tolist()\n",
    "print(\"Column names:\\n\")\n",
    "for j in col_names:\n",
    "    print(j)\n",
    "to_show = col_names[:6] + col_names[-6:]\n",
    "print(\"\\nSample data:\")\n",
    "churn_df[to_show].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d747a6-30a4-461b-83ac-89eeabfbf0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate target data\n",
    "churn_result = churn_df['Churn?']\n",
    "y = np.where(churn_result == 'True.',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d92db-797e-43c2-9142-786340816965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need these columns\n",
    "to_drop = ['State','Area Code','Phone','Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6854f-605a-4b14-b1a0-77e4d6eb7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42f003-50f5-48bc-babf-0fcb02024067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out features for future use\n",
    "features = churn_feat_space.columns\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e580f-bec4-46af-8482-96857a8f1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn_feat_space\n",
    "# This is important\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(\"Feature space holds %d observations and %d features\" % X.shape)\n",
    "print(\"Unique target labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4050f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d562ad8-84f2-488d-a59c-0d9d71c0ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(n_splits=3,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    # Iterate through folds\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd7659-0605-4c9e-a449-e927737aa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,LR)))\n",
    "print(\"\\nGradient Boosting Classifier\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,GBC)))\n",
    "print(\"\\nSupport vector machines:\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,SVC)))\n",
    "print(\"\\nRandom forest:\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,RF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfae76a-c1e7-4752-aef7-d253c342a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrices(confusion_matricies,class_names):\n",
    "    class_names = class_names.tolist()\n",
    "    for cm in confusion_matrices:\n",
    "        classifier, cm = cm[0], cm[1]\n",
    "        print(cm)\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(cm)\n",
    "        plt.title('Confusion matrix for %s' % classifier)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + class_names)\n",
    "        ax.set_yticklabels([''] + class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "class_names = np.unique(y)\n",
    "\n",
    "confusion_matrices = [\n",
    "    ( \"Support Vector Machines\", confusion_matrix(y,run_cv(X,y,SVC)) ),\n",
    "    ( \"Random Forest\", confusion_matrix(y,run_cv(X,y,RF)) ),\n",
    "    ( \"Gradient Boosting Classifier\", confusion_matrix(y,run_cv(X,y,GBC)) ),\n",
    "    ( \"Logisitic Regression\", confusion_matrix(y,run_cv(X,y,LR)) )\n",
    "]\n",
    "\n",
    "%matplotlib inline\n",
    "draw_confusion_matrices(confusion_matrices,class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e14572-76bb-4c5f-8296-8bdf81c4e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(X, y, clf_class, **kwargs):\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_index], y_prob[test_index, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    mean_tpr /= kf.get_n_splits(X)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support vector machines:\")\n",
    "plot_roc(X,y,SVC,probability=True)\n",
    "\n",
    "print(\"Random forests:\")\n",
    "plot_roc(X,y,RF,n_estimators=18)\n",
    "\n",
    "print(\"Gradient Boosting Classifier:\")\n",
    "plot_roc(X,y,GBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b11da6b",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb69bdf-8112-494a-ae8d-ef902439843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index,test_index = train_test_split(churn_df.index)\n",
    "\n",
    "forest = RF()\n",
    "forest_fit = forest.fit(X[train_index], y[train_index])\n",
    "forest_predictions = forest_fit.predict(X[test_index])\n",
    "\n",
    "importances = forest_fit.feature_importances_[:10]\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(10), importances[indices], yerr=std[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(10), indices)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71ed96-e64c-48d4-989a-07f3176c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prob_cv(X, y, clf_class, roc=False, **kwargs):\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "    return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476cd8a0-6a5d-4535-8c63-d99b84090598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 10 estimators so predictions are all multiples of 0.1\n",
    "pred_prob = run_prob_cv(X, y, RF, n_estimators=10)\n",
    "pred_churn = pred_prob[:,1]\n",
    "is_churn = y == 1\n",
    "\n",
    "# Number of times a predicted probability is assigned to an observation\n",
    "counts = pd.value_counts(pred_churn)\n",
    "counts[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf7aa6-8ec8-47d1-b01d-85fc5125eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_prob = defaultdict(float)\n",
    "\n",
    "# calculate true probabilities\n",
    "for prob in counts.index:\n",
    "    true_prob[prob] = np.mean(is_churn[pred_churn == prob])\n",
    "true_prob = pd.Series(true_prob)\n",
    "\n",
    "# pandas-fu\n",
    "counts = pd.concat([counts,true_prob], axis=1).reset_index()\n",
    "counts.columns = ['pred_prob', 'count', 'true_prob']\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8276dc-f10a-4d36-82aa-2cb969057a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Measurements inspired by Philip Tetlock's \"Expert Political Judgment\"\n",
    "\n",
    "Equations take from Yaniv, Yates, & Smith (1991):\n",
    "  \"Measures of Descrimination Skill in Probabilistic Judgement\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def calibration(prob,outcome,n_bins=10):\n",
    "    \"\"\"Calibration measurement for a set of predictions.\n",
    "\n",
    "    When predicting events at a given probability, how far is frequency\n",
    "    of positive outcomes from that probability?\n",
    "    NOTE: Lower scores are better\n",
    "\n",
    "    prob: array_like, float\n",
    "        Probability estimates for a set of events\n",
    "\n",
    "    outcome: array_like, bool\n",
    "        If event predicted occurred\n",
    "\n",
    "    n_bins: int\n",
    "        Number of judgement categories to prefrom calculation over.\n",
    "        Prediction are binned based on probability, since \"descrete\" \n",
    "        probabilities aren't required. \n",
    "\n",
    "    \"\"\"\n",
    "    prob = np.array(prob)\n",
    "    outcome = np.array(outcome)\n",
    "\n",
    "    c = 0.0\n",
    "    # Construct bins\n",
    "    judgement_bins = np.arange(n_bins + 1) / n_bins\n",
    "    # Which bin is each prediction in?\n",
    "    bin_num = np.digitize(prob,judgement_bins)\n",
    "    for j_bin in np.unique(bin_num):\n",
    "        # Is event in bin\n",
    "        in_bin = bin_num == j_bin\n",
    "        # Predicted probability taken as average of preds in bin\n",
    "        predicted_prob = np.mean(prob[in_bin])\n",
    "        # How often did events in this bin actually happen?\n",
    "        true_bin_prob = np.mean(outcome[in_bin])\n",
    "        # Squared distance between predicted and true times num of obs\n",
    "        c += np.sum(in_bin) * ((predicted_prob - true_bin_prob) ** 2)\n",
    "    return c / len(prob)\n",
    "\n",
    "def discrimination(prob,outcome,n_bins=10):\n",
    "    \"\"\"Discrimination measurement for a set of predictions.\n",
    "\n",
    "    For each judgement category, how far from the base probability\n",
    "    is the true frequency of that bin?\n",
    "    NOTE: High scores are better\n",
    "\n",
    "    prob: array_like, float\n",
    "        Probability estimates for a set of events\n",
    "\n",
    "    outcome: array_like, bool\n",
    "        If event predicted occurred\n",
    "\n",
    "    n_bins: int\n",
    "        Number of judgement categories to prefrom calculation over.\n",
    "        Prediction are binned based on probability, since \"descrete\" \n",
    "        probabilities aren't required. \n",
    "\n",
    "    \"\"\"\n",
    "    prob = np.array(prob)\n",
    "    outcome = np.array(outcome)\n",
    "\n",
    "    d = 0.0\n",
    "    # Base frequency of outcomes\n",
    "    base_prob = np.mean(outcome)\n",
    "    # Construct bins\n",
    "    judgement_bins = np.arange(n_bins + 1) / n_bins\n",
    "    # Which bin is each prediction in?\n",
    "    bin_num = np.digitize(prob,judgement_bins)\n",
    "    for j_bin in np.unique(bin_num):\n",
    "        in_bin = bin_num == j_bin\n",
    "        true_bin_prob = np.mean(outcome[in_bin])\n",
    "        # Squared distance between true and base times num of obs\n",
    "        d += np.sum(in_bin) * ((true_bin_prob - base_prob) ** 2)\n",
    "    return d / len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15324eff-98a1-48a0-aa5d-52ff5b345269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measurements(pred_prob):\n",
    "    churn_prob, is_churn = pred_prob[:,1], y == 1\n",
    "    print(\"  %-20s %.4f\" % (\"Calibration Error\", calibration(churn_prob, is_churn)))\n",
    "    print(\"  %-20s %.4f\" % (\"Discrimination\", discrimination(churn_prob,is_churn)))\n",
    "    print(\"Note -- Lower calibration is better, higher discrimination is better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec194a4-93ca-4830-8fc5-12ab300e217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support vector machines:\")\n",
    "print_measurements(run_prob_cv(X,y,SVC,probability=True))\n",
    "\n",
    "print(\"\\nRandom forests:\")\n",
    "print_measurements(run_prob_cv(X,y,RF,n_estimators=18))\n",
    "\n",
    "print(\"\\nGradient Boosting Classifier:\")\n",
    "print_measurements(run_prob_cv(X,y,GBC))\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print_measurements(run_prob_cv(X,y,RF))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
